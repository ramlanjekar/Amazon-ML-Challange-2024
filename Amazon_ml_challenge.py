# -*- coding: utf-8 -*-
"""Amazon_ML_Challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNq5KIuCisYrsI2TJ61t7ZUahrxnIBOd
"""

from google.colab import drive
drive.mount('/content/drive')

import re
import os
import requests
import pandas as pd
import multiprocessing
import time
from time import time as timer
from tqdm import tqdm
import numpy as np
from pathlib import Path
from functools import partial
import urllib
from PIL import Image
from concurrent.futures import ThreadPoolExecutor, as_completed

processed_df.head()

!pip install paddlepaddle-gpu
!pip install "paddleocr>=2.0.1"

import pandas as pd
from paddleocr import PaddleOCR
from tqdm.auto import tqdm

# Enable tqdm progress bar for pandas
tqdm.pandas()

# Initialize PaddleOCR with GPU
ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=True)

def extract_text_from_image(image_path):
    """Extracts text from an image using PaddleOCR."""
    try:
        result = ocr.ocr(image_path, cls=True)
        # Concatenate all detected text
        text = ' '.join([line[1][0] for line in result[0]])
        return text if text else "No text detected"
    except Exception as e:
        return f"Error processing {image_path}: {str(e)}"

def process_dataframe_with_ocr(df):
    print("Extracting text from images using PaddleOCR...")
    df['extracted_text'] = df['local_image_path'].progress_apply(extract_text_from_image)
    return df

# Assuming you have already created processed_df
processed_df = process_dataframe_with_ocr(processed_df)

print("Text extraction complete. 'extracted_text' column added to processed_df.")

!pip install spacy[cuda]

!pip install swifter

import spacy
from tqdm.auto import tqdm
import re
import string
from textblob import TextBlob
import swifter
import time

# Enable tqdm progress bar for pandas
tqdm.pandas()

# Load the spaCy model with GPU support and enable batch processing
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

def separate_number_and_unit(text):
    """
    Separate numbers from units if they are attached, e.g., '200g' -> '200 g'.
    """
    return re.sub(r'(\d)([a-zA-Z]+)', r'\1 \2', text)

def clean_and_remove_stopwords(doc):
    """
    Preserve special characters, remove generic stop words, retain numbers (including decimals, exponents),
    and numbers followed by any units. Prevent duplication of numbers and their units.
    """
    # Regex pattern to match numbers including decimals and exponents
    number_pattern = re.compile(r'^-?\d+(\.\d+)?([eE][-+]?\d+)?$')

    # Regex pattern to match numerical value followed by a word (e.g., '0.709 g')
    number_with_unit_pattern = re.compile(r'^-?\d+(\.\d+)?([eE][-+]?\d+)?\s*[a-zA-Z]+$')

    cleaned_text = []
    skip_next = False

    for i, token in enumerate(doc):
        if skip_next:
            skip_next = False
            continue

        token_text = token.text.lower()  # Convert to lowercase here

        # Check if the token matches the pattern for number
        if number_pattern.match(token_text):
            if i + 1 < len(doc) and number_with_unit_pattern.match(token_text + ' ' + doc[i + 1].text):
                cleaned_text.append(token_text + ' ' + doc[i + 1].text.lower())  # Convert next token to lowercase
                skip_next = True
            else:
                cleaned_text.append(token_text)
        # Preserve alphanumeric characters and spaces
        elif not token.is_stop:
            if token.is_alpha or token_text.isspace() or token_text.isalnum():
                cleaned_text.append(token_text)  # Already converted to lowercase

    # Join the tokens into a string and separate numbers from units if attached
    cleaned_text = ' '.join(cleaned_text)
    cleaned_text = separate_number_and_unit(cleaned_text)

    # Remove duplicate numbers
    cleaned_text = re.sub(r'(\b\d+(\.\d+)?\s+[a-zA-Z]+\s*)+\b\d+(\.\d+)?', r'\1', cleaned_text)

    return cleaned_text

def remove_non_standard_chars(text):
    # Remove punctuation except for the decimal point
    punct_to_remove = string.punctuation.replace('.', '')
    translator = str.maketrans('', '', punct_to_remove + '+-#$')

    # Use regex to ensure we're only preserving decimal points, not other uses of periods
    text = re.sub(r'(?<!\d)\.(?!\d)', '', text)  # Remove periods that aren't between digits

    return text.translate(translator)

def correct_spelling(text):
    """
    Correct spelling in the text using TextBlob.
    """
    return str(TextBlob(text).correct())

def clean_text(text):
    # Combine all cleaning steps in one function
    doc = nlp(text)  # Process using spaCy's model
    cleaned_text = clean_and_remove_stopwords(doc)
    cleaned_text = remove_non_standard_chars(cleaned_text)
    # cleaned_text = correct_spelling(cleaned_text)
    return cleaned_text

def process_dataframe_clean_text(df):
    print("Cleaning extracted text...")

    # Start timing the process
    start_time = time.time()

    # Enable parallel apply using swifter
    df['cleaned_text'] = df['extracted_text'].swifter.apply(clean_text)

    # End timing the process
    end_time = time.time()

    # Calculate the time taken and print it
    time_taken = end_time - start_time
    print(f"Time taken to clean the text: {time_taken:.2f} seconds")

    return df

# Assuming you have already created processed_df with 'extracted_text' column
processed_df = process_dataframe_clean_text(processed_df)

print("Text cleaning complete. 'cleaned_text' column added to processed_df.")

processed_df['cleaned_text']

import re
from collections import defaultdict
import pandas as pd
import dask.dataframe as dd
from dask.distributed import Client
from tqdm import tqdm

class TrieNode:
    def __init__(self):
        self.children = defaultdict(TrieNode)
        self.is_end = False
        self.standard_unit = None

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word, standard_unit):
        node = self.root
        for char in word:
            node = node.children[char]
        node.is_end = True
        node.standard_unit = standard_unit

    def find_longest_prefix(self, word):
        node = self.root
        last_match = None
        last_match_length = 0
        for i, char in enumerate(word):
            if char not in node.children:
                break
            node = node.children[char]
            if node.is_end:
                last_match = node.standard_unit
                last_match_length = i + 1
        return last_match, last_match_length

# Define unit mappings
unit_mappings = {
    'item_weight': {
        'gram': {'gram', 'g', 'gm', 'grams', 'gr', 'grm', 'grms'},
        'kilogram': {'kilogram', 'kg', 'kgs', 'kilograms', 'kilo', 'kilos'},
        'microgram': {'microgram', 'µg', 'mcg', 'micrograms'},
        'milligram': {'milligram', 'mg', 'mgs', 'milligrams', 'milli-gram'},
        'ounce': {'ounce', 'oz', 'ounces'},
        'pound': {'pound', 'lb', 'lbs', 'pounds'},
        'ton': {'ton', 't', 'tons', 'tonne', 'tonnes'}
    },
    'width': {
        'centimetre': {'centimetre', 'cm', 'centimeters', 'centimeter', 'cms'},
        'foot': {'foot', 'ft', 'feet', "'"},
        'inch': {'inch', 'in', '"', 'inches'},
        'metre': {'metre', 'm', 'meter', 'meters'},
        'millimetre': {'millimetre', 'mm', 'millimeter', 'millimeters', 'mms'},
        'yard': {'yard', 'yd', 'yards'}
    },
    'depth': {
        'centimetre': {'centimetre', 'cm', 'centimeters', 'centimeter', 'cms'},
        'foot': {'foot', 'ft', 'feet', "'"},
        'inch': {'inch', 'in', '"', 'inches'},
        'metre': {'metre', 'm', 'meter', 'meters'},
        'millimetre': {'millimetre', 'mm', 'millimeter', 'millimeters', 'mms'},
        'yard': {'yard', 'yd', 'yards'}
    },
    'height': {
        'centimetre': {'centimetre', 'cm', 'centimeters', 'centimeter', 'cms'},
        'foot': {'foot', 'ft', 'feet', "'"},
        'inch': {'inch', 'in', '"', 'inches'},
        'metre': {'metre', 'm', 'meter', 'meters'},
        'millimetre': {'millimetre', 'mm', 'millimeter', 'millimeters', 'mms'},
        'yard': {'yard', 'yd', 'yards'}
    },
    'maximum_weight_recommendation': {
        'gram': {'gram', 'g', 'gm', 'grams', 'gr', 'grm', 'grms'},
        'kilogram': {'kilogram', 'kg', 'kgs', 'kilograms', 'kilo', 'kilos'},
        'microgram': {'microgram', 'µg', 'mcg', 'micrograms'},
        'milligram': {'milligram', 'mg', 'mgs', 'milligrams', 'milli-gram'},
        'ounce': {'ounce', 'oz', 'ounces'},
        'pound': {'pound', 'lb', 'lbs', 'pounds'},
        'ton': {'ton', 't', 'tons', 'tonne', 'tonnes'}
    },
    'voltage': {
        'kilovolt': {'kilovolt', 'kv', 'kilovolts', 'kilo-volt'},
        'millivolt': {'millivolt', 'mv', 'millivolts'},
        'volt': {'volt', 'v', 'volts'}
    },
    'wattage': {
        'kilowatt': {'kilowatt', 'kw', 'kilowatts', 'kilo-watt'},
        'watt': {'watt', 'w', 'watts'}
    },
    'item_volume': {
        'centilitre': {'centilitre', 'cl', 'centilitres', 'centiliter', 'centiliters'},
        'cubic foot': {'cubic foot', 'ft³', 'cu ft', 'ft^3', 'cubic feet'},
        'cubic inch': {'cubic inch', 'in³', 'cu in', 'in^3', 'cubic inches'},
        'cup': {'cup', 'cups'},
        'decilitre': {'decilitre', 'dl', 'deciliter', 'decilitres', 'deciliters'},
        'fluid ounce': {'fluid ounce', 'fl oz', 'fl. oz.', 'fluid ounces', 'fluid-oz'},
        'gallon': {'gallon', 'gal', 'gallons'},
        'imperial gallon': {'imperial gallon', 'imp gal', 'imp gallons', 'imperial gallons'},
        'litre': {'litre', 'l', 'litres', 'liter', 'liters'},
        'microlitre': {'microlitre', 'µl', 'microliters', 'microliter', 'microlitres'},
        'millilitre': {'millilitre', 'ml', 'milliliters', 'millilitres', 'milli-litre', 'milli-liter'},
        'pint': {'pint', 'pt', 'pints'},
        'quart': {'quart', 'qt', 'quarts'}
    }
}

def build_trie(unit_mappings):
    trie = Trie()
    for entity, units in unit_mappings.items():
        for standard_unit, variations in units.items():
            for variation in variations:
                trie.insert(variation.lower(), standard_unit)
    return trie

unit_trie = build_trie(unit_mappings)

def convert_units(text, entity_name):
    words = text.split()
    result = []
    i = 0
    while i < len(words):
        match, length = unit_trie.find_longest_prefix(words[i].lower())
        if match:
            result.append(match)
            i += 1
        else:
            result.append(words[i])
            i += 1
    return ' '.join(result)

def process_row(row):
    text = row['cleaned_text']
    entity_name = row['entity_name']
    processed_text = convert_units(text, entity_name)
    return processed_text

def process_dataframe(df, num_partitions=100):
    ddf = dd.from_pandas(df, npartitions=num_partitions)
    processed_texts = ddf.apply(process_row, axis=1, meta=('processed_text', 'object'))
    return processed_texts.compute()

# Usage
if __name__ == '__main__':
    # Set up Dask client
    client = Client()  # This will use all available cores by default

    # Assuming 'df' is your pandas DataFrame with columns 'extracted_text' and 'entity_name'
    # df = pd.read_csv('your_data.csv')  # Load your data

    # Process the DataFrame
    processed_df['processed_text'] = process_dataframe(processed_df)

    # Save the result
    # df.to_csv('processed_data.csv', index=False)

    # Close the Dask client
    client.close()

pip install dask[complete]

import re
import pandas as pd

def extract_value_and_unit(entity):
    """
    Extract numeric value and unit from the entity string.
    """
    match = re.search(r'(\d+(?:\.\d+)?)\s*(\w+)', str(entity))
    if match:
        value, unit = match.groups()
        return int(float(value)), unit.lower()
    return None, None

def find_match_in_text(row):
    """
    Find if the entity value and unit appear in the processed text.
    """
    if pd.isna(row['entity_name']) or pd.isna(row['processed_text']):
        return False

    value, unit = row['temp_value_unit']
    if value is None or unit is None:
        return False

    text = str(row['processed_text']).lower()
    value_pattern = r'\b' + str(value) + r'\b'
    value_matches = list(re.finditer(value_pattern, text))

    for match in value_matches:
        end_pos = match.end()
        next_word = re.search(r'\b\w+\b', text[end_pos:])
        if next_word and next_word.group().lower() == unit:
            return True

    return False

def count_matching_rows(df):
    """
    Count how many rows in the DataFrame have a match between entity_value and processed_text.
    """
    # Create temporary column with value and unit
    df['temp_value_unit'] = df['entity_name'].apply(extract_value_and_unit)

    # Perform the matching
    df['match'] = df.apply(find_match_in_text, axis=1)

    # Count matches
    total_matches = df['match'].sum()

    # Clean up temporary column
    df.drop('temp_value_unit', axis=1, inplace=True)

    return total_matches, len(df)

# Assuming processed_df is your DataFrame
matched_count, total_count = count_matching_rows(processed_df)

print(f"Number of matches: {matched_count}")
print(f"Total rows: {total_count}")
print(f"Percentage of matches: {(matched_count / total_count) * 100:.2f}%")

processed_df['extracted_window']

pd.set_option('display.max_colwidth', None)
processed_df['processed_text'].head(20)

pip install dask

import dask
import dask.dataframe as dd
import pandas as pd
import time

# Sample DataFrame creation (assuming `processed_df` is a pandas DataFrame)
# processed_df = pd.DataFrame({'processed_text': ["This is a sample text.", "Another example sentence."]})

# Load the DataFrame into Dask
ddf = dd.from_pandas(processed_df, npartitions=4)  # Adjust partitions as needed

# Compute the length of each row (since text is already split into words)
ddf['text_length'] = ddf['processed_text'].apply(len, meta=('x', 'int64'))

# Measure time taken to count rows with specific word lengths
start_time = time.time()

# Filter rows with fewer than 250 words
count_less_than_250 = ddf[ddf['text_length'] < 250].shape[0].compute()

# Filter rows with more than 500 words
count_more_than_500 = ddf[ddf['text_length'] > 500].shape[0].compute()

end_time = time.time()

print(f"Number of rows with fewer than 250 words: {count_less_than_250}")
print(f"Number of rows with more than 500 words: {count_more_than_500}")
print(f"Time taken to compute: {end_time - start_time} seconds")

import dask
import dask.dataframe as dd
import pandas as pd
import time

# Sample DataFrame creation (assuming `processed_df` is a pandas DataFrame)
# processed_df = pd.DataFrame({
#     'groupid': [1, 2],
#     'entity_name': ['entity1', 'entity2'],
#     'processed_text': ['text1', 'text2']
# })

# Load the DataFrame into Dask
ddf = dd.from_pandas(processed_df, npartitions=4)  # Adjust partitions as needed

# Define a function to create the input text
def create_input(entity_name, groupid, processed_text):
    return f"[PREDICT] {entity_name} [CATEGORY] {groupid} [FROM TEXT] {processed_text}"

# Apply the function to create the new 'input' column
ddf['input'] = ddf.apply(lambda row: create_input(row['entity_name'], row['group_id'], row['processed_text']),
                        axis=1, meta=('x', 'str'))

# Measure time taken to compute the new column
start_time = time.time()

# Compute the DataFrame with the new column
processed_df_with_input = ddf.compute()

end_time = time.time()



print(f"Time taken to compute the new column: {end_time - start_time} seconds")

processed_df_with_input.head()

# Function to clean and standardize 'entity_name'
def clean_entity_name(entity_name):
    match = re.match(r'(\d+)\s*(\w+)(?:\s*to\s*(\d+)\s*(\w+))?', entity_name)
    if match:
        value1 = int(match.group(1))
        unit1 = match.group(2)
        value2 = match.group(3)
        unit2 = match.group(4)
        if value2 and unit2 and unit1 == unit2:
            value1 = (value1 + int(value2)) / 2
        return f'{int(value1)} {unit1}'
    return entity_name

# Apply the cleaning function using swifter
processed_df_with_input['cleaned_entity_name'] = processed_df_with_input['entity_name'].swifter.apply(clean_entity_name)

print(processed_df_with_input)

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import numpy as np

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.tokenizer = tokenizer
        self.texts = texts
        self.labels = labels
        self.max_length = max_length

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        # Prepare input
        input_text = f"{text} [EXTRACT] {label}"
        encodings = self.tokenizer(input_text, truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')

        input_ids = encodings['input_ids'].squeeze()
        attention_mask = encodings['attention_mask'].squeeze()

        # Prepare labels for language modeling
        labels = input_ids.clone()
        # Mask out the labels for the input part (we only want to predict the extraction part)
        extract_token_idx = (input_ids == self.tokenizer.encode("[EXTRACT]", add_special_tokens=False)[0]).nonzero(as_tuple=True)[0]
        labels[:extract_token_idx+1] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

    def __len__(self):
        return len(self.texts)

# Prepare your data
texts = processed_df_with_input['input'].tolist()
labels = processed_df_with_input['cleaned_entity_name'].tolist()

# Split the data
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.1, random_state=42)

# Create datasets
train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=350)
val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=350)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

# Load model
model = GPT2LMHeadModel.from_pretrained('gpt2')

def move_model_to_device(model, device):
    # Check if model is already on the target device
    if next(model.parameters()).device != device:
        print(f"Moving model to {device}")
        model.to(device)
    else:
        print(f"Model is already on {device}")

move_model_to_device(model, device)

# Optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=5e-5)
total_steps = len(train_loader) * 10  # 10 epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

# Mixed precision training
scaler = torch.cuda.amp.GradScaler()

# Training loop
from torch.cuda.amp import autocast, GradScaler

def train(model, train_loader, val_loader, optimizer, scheduler, num_epochs, accumulation_steps=4):
    scaler = GradScaler()
    best_val_loss = float('inf')
    for epoch in range(num_epochs):
        model.train()
        total_train_loss = 0
        optimizer.zero_grad()

        for i, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}")):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            with autocast():
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss / accumulation_steps

            scaler.scale(loss).backward()

            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                scheduler.step()

            total_train_loss += loss.item() * accumulation_steps

        avg_train_loss = total_train_loss / len(train_loader)
        print(f"Average training loss: {avg_train_loss:.4f}")

        # Validation
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                with autocast():
                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss

                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        print(f"Validation loss: {avg_val_loss:.4f}")

        # Save model if it's the best so far
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            model_save_path = f'model_epoch_{epoch+1}.pt'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_val_loss,
            }, model_save_path)
            print(f"Model saved to {model_save_path}")

# Update batch sizes
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)

# Update optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=5e-5)
total_steps = len(train_loader) * 10 // 4  # 10 epochs, divided by accumulation steps
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

# Run training
train(model, train_loader, val_loader, optimizer, scheduler, num_epochs=10, accumulation_steps=4)

# Evaluation function
def evaluate(model, val_loader, tokenizer):
    model.eval()
    all_predictions = []
    all_true_labels = []
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluation"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            generated = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_length=input_ids.shape[1] + 50,
                num_return_sequences=1,
                no_repeat_ngram_size=2,
                early_stopping=True,
                pad_token_id=tokenizer.eos_token_id  # Use EOS token for padding
            )

            for gen, input_id in zip(generated, input_ids):
                extract_token_idx = (input_id == tokenizer.encode("[EXTRACT]", add_special_tokens=False)[0]).nonzero(as_tuple=True)[0]
                if len(extract_token_idx) > 0:
                    extract_token_idx = extract_token_idx.item()
                    gen_text = tokenizer.decode(gen[extract_token_idx+1:], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()
                    true_text = tokenizer.decode(input_id[extract_token_idx+1:], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()
                    all_predictions.append(gen_text)
                    all_true_labels.append(true_text)

    # Calculate accuracy
    correct = sum(pred == true for pred, true in zip(all_predictions, all_true_labels))
    accuracy = correct / len(all_predictions) if all_predictions else 0

    print(f"Accuracy: {accuracy:.4f}")
    print("Sample predictions:")
    for pred, true in zip(all_predictions[:10], all_true_labels[:10]):
        print(f"Predicted: {pred} | True: {true}")

# Set padding side to left for generation
tokenizer.padding_side = 'left'

# Run evaluation
evaluate(model, val_loader, tokenizer)

# Run evaluation
evaluate(model, val_loader, tokenizer)

# Function for inference
def extract_measurement(model, tokenizer, text):
    input_text = f"{text} [EXTRACT] "
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=350).to(device)

    with torch.no_grad():
        generated = model.generate(
            **inputs,
            max_length=inputs['input_ids'].shape[1] + 50,
            num_return_sequences=1,
            no_repeat_ngram_size=2,
            early_stopping=True
        )

    return tokenizer.decode(generated[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()

# Example usage of inference function
sample_text = "The package contains 50 kilogram of flour for baking."
extracted = extract_measurement(model, tokenizer, sample_text)
print(f"Extracted measurement: {extracted}")

import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

# Load the best model
best_model_path = 'model_epoch_10.pt'  # Adjust this to the actual best model path
model = GPT2LMHeadModel.from_pretrained('gpt2')
checkpoint = torch.load("/content/model_epoch_9.pt")
model.load_state_dict(checkpoint['model_state_dict'])
model.to(device)
model.eval()

# Define a function for inference
def extract_measurement(model, tokenizer, text):
    input_text = f"{text} [EXTRACT] "
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=350).to(device)

    with torch.no_grad():
        generated = model.generate(
            **inputs,
            max_length=inputs['input_ids'].shape[1] + 50,
            num_return_sequences=1,
            no_repeat_ngram_size=2,
            early_stopping=True
        )

    return tokenizer.decode(generated[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()

# Example row from the training data
example_index = 0  # Replace with your desired index
text = processed_df_with_input['input'][example_index]
group_id = processed_df_with_input['group_id'][example_index]
entity_name = processed_df_with_input['entity_name'][example_index]

# Print out the example data
print(f"Example Text: {text}")
print(f"Group ID: {group_id}")
print(f"Entity Name: {entity_name}")

# Make prediction
predicted_value = extract_measurement(model, tokenizer, text)
print(f"Predicted Value: {predicted_value}")

# Install necessary libraries if not already installed
!pip install swifter dask

# import pandas as pd
# import re
# import swifter  # For parallelized and optimized apply functions
# from sklearn.preprocessing import LabelEncoder

# # Assuming processed_df already exists with an 'entity_value' column

# def extract_value_and_unit(entity_value):
#     """
#     Extracts the numeric value and the unit from a string.
#     """
#     # Use regular expressions to extract the numeric value and the unit
#     match = re.match(r"([0-9.]+)\s*(\w+)", entity_value)
#     if match:
#         value = float(match.group(1))  # Convert the numeric part to float
#         unit = match.group(2)  # Extract the unit
#         return value, unit
#     return None, None

# # Apply the extraction function in parallel using swifter
# processed_df[['entity_value_numeric', 'entity_unit']] = processed_df['entity_value'].swifter.apply(
#     lambda x: pd.Series(extract_value_and_unit(x)))

# # Encode the entity units directly for classification task using LabelEncoder
# label_encoder = LabelEncoder()
# processed_df['entity_unit_encoded'] = label_encoder.fit_transform(processed_df['entity_unit'])


# # Now we only have two columns added: 'entity_value_numeric' and 'entity_unit_encoded'
# # Check the result
# print(processed_df[['entity_value_numeric', 'entity_unit_encoded']].head())

pip install pytorch-crf

import pandas as pd

# Sample DataFrame for demonstration
# processed_df = pd.DataFrame({'entity_value': ['value1', 123, 'value2', None, 'value3']})

# Check if all values in 'entity_value' are strings
def check_string_values(df, column_name):
    non_string_count = df[~df[column_name].apply(lambda x: isinstance(x, str))].shape[0]
    return non_string_count

# Apply the function
non_string_count = check_string_values(processed_df, 'processed_text')

# Print the result
print(f"Number of non-string values in 'entity_value': {non_string_count}")

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from torchcrf import CRF
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from tqdm import tqdm

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# ADD THIS AT THE BEGINNING:
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")

# ADD THIS FUNCTION:
def print_gpu_memory():
    if torch.cuda.is_available():
        print(f"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
        print(f"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
        print(f"Max GPU memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")

# ADD THIS FUNCTION:
def inspect_batch(batch):
    print(f"Input IDs shape: {batch['input_ids'].shape}")
    print(f"Attention Mask shape: {batch['attention_mask'].shape}")
    print(f"Labels shape: {batch['labels'].shape}")
    print(f"Sample Input IDs: {batch['input_ids'][0][:10]}")
    print(f"Sample Attention Mask: {batch['attention_mask'][0][:10]}")
    print(f"Sample Labels: {batch['labels'][0][:10]}")


# 1. Data Preparation
class EntityExtractionDataset(Dataset):
    def __init__(self, texts, entities, tokenizer, max_length=128):
        self.texts = texts
        self.entities = entities
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        entity = self.entities[idx]

        encoding = self.tokenizer(text,
                                  truncation=True,
                                  padding=False,
                                  max_length=self.max_length,
                                  return_tensors='pt')

        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)

        # Create labels
        labels = torch.zeros(len(input_ids), dtype=torch.long)  # 0 for 'O'
        entity_tokens = self.tokenizer.tokenize(entity)
        text_tokens = self.tokenizer.tokenize(text)

        entity_start = None
        for i in range(len(text_tokens) - len(entity_tokens) + 1):
            if text_tokens[i:i+len(entity_tokens)] == entity_tokens:
                entity_start = i
                break

        if entity_start is not None:
            labels[entity_start+1] = 1  # B-ENTITY
            for i in range(entity_start+2, min(entity_start+1+len(entity_tokens), len(input_ids))):
                labels[i] = 2  # I-ENTITY

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

# Custom collate function for dynamic padding
def collate_fn(batch):
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
    labels = [item['labels'] for item in batch]

    # Pad sequences to the maximum length in the batch
    input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_mask = nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)  # Use -100 for label padding

    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels
    }

# 2. Model Architecture
class BertCRFModel(nn.Module):
    def __init__(self, num_labels):
        super(BertCRFModel, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.crf = CRF(num_labels, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        emissions = self.fc(sequence_output)

        if labels is not None:
            mask = attention_mask.bool()
            # Replace -100 with 0 in labels for CRF
            labels = torch.where(labels == -100, torch.zeros_like(labels), labels)
            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')
            return loss
        else:
            return self.crf.decode(emissions, mask=attention_mask.bool())

# 3. Training and Evaluation Functions
import torch
from tqdm import tqdm

def train(model, train_loader, optimizer, device, epoch):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Training Epoch {epoch}"):
        # Check for NaN or Inf in the batch
        for k, v in batch.items():
            if torch.isnan(v).any() or torch.isinf(v).any():
                print(f"NaN or Inf detected in {k}")
                continue

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Print shapes for debugging
        print(f"Input IDs shape: {input_ids.shape}")
        print(f"Attention Mask shape: {attention_mask.shape}")
        print(f"Labels shape: {labels.shape}")

        optimizer.zero_grad()
        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

        # Check for NaN or Inf in the loss
        if torch.isnan(loss) or torch.isinf(loss):
            print("NaN or Inf detected in loss!")
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(train_loader)


def evaluate(model, test_loader, device):
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels']

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            predictions.extend(outputs)
            true_labels.extend(labels.tolist())

    # Flatten the lists and remove padding tokens
    flat_preds = [p for pred in predictions for p in pred]
    flat_labels = [l for label in true_labels for l in label if l != -100]

    return f1_score(flat_labels, flat_preds, average='weighted')

def convert_to_entities(tokens, labels):
    entities = []
    current_entity = []
    for token, label in zip(tokens, labels):
        if label == 1:  # B-ENTITY
            if current_entity:
                entities.append(' '.join(current_entity))
                current_entity = []
            current_entity.append(token)
        elif label == 2:  # I-ENTITY
            current_entity.append(token)
        elif label == 0:  # O
            if current_entity:
                entities.append(' '.join(current_entity))
                current_entity = []
    if current_entity:
        entities.append(' '.join(current_entity))
    return entities

# 4. Main Execution
def main():
    # Assuming processed_df has 'processed_text' and 'entity_value' columns
    texts = processed_df['processed_text'].tolist()
    entities = processed_df['entity_value'].tolist()

    # Split the data
    train_texts, test_texts, train_entities, test_entities = train_test_split(
        texts, entities, test_size=0.2, random_state=42)

    # Initialize tokenizer and create datasets
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    train_dataset = EntityExtractionDataset(train_texts, train_entities, tokenizer)
    test_dataset = EntityExtractionDataset(test_texts, test_entities, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)

    # ADD THIS:
    for batch in train_loader:
        inspect_batch(batch)
        break

    # Initialize model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = BertCRFModel(num_labels=3)
    model = model

    # Setup optimizer
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)

    # Training loop
    num_epochs = 5
    for epoch in range(num_epochs):
        print_gpu_memory()  # ADD THIS
        avg_loss = train(model, train_loader, optimizer, device, epoch)
        print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}")

        # Evaluate on test set
        f1 = evaluate(model, test_loader, device)
        print(f"Epoch {epoch+1}/{num_epochs}, F1 Score: {f1:.4f}")

    # ADD THIS: Example of using the model for prediction
    model.eval()
    sample_text = "Your sample text here"
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    inputs = tokenizer(sample_text, return_tensors='pt', truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    predicted_labels = outputs[0]
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    entities = convert_to_entities(tokens, predicted_labels)
    print(f"Extracted entities: {entities}")


if __name__ == "__main__":
    main()

pip install torchcrf



# import pandas as pd
# from sentence_transformers import SentenceTransformer
# from tqdm.auto import tqdm
# import time
# import numpy as np
# import torch

# # Enable tqdm progress bar for pandas
# tqdm.pandas()

# # Load the SBERT model
# model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device='cuda')

# def generate_embeddings(texts, model, batch_size=32):
#     """
#     Generate embeddings for a list of texts using the provided model.
#     """
#     return model.encode(texts, show_progress_bar=True, device='cuda', batch_size=batch_size)

# def process_dataframe_with_embeddings(df, model, batch_size=32):
#     """
#     Add embeddings to the DataFrame and measure the time taken.
#     """
#     print("Generating embeddings for cleaned text...")

#     # Capture the start time
#     start_time = time.time()

#     # Generate embeddings for the 'cleaned_text' column
#     embeddings = generate_embeddings(df['cleaned_text'].tolist(), model, batch_size)

#     # Convert embeddings to a list of numpy arrays
#     df['embeddings'] = list(embeddings)

#     # Capture the end time
#     end_time = time.time()

#     # Calculate the time taken
#     duration = end_time - start_time
#     print(f"Time taken for embedding generation: {duration:.2f} seconds")
#     print(f"Average time per sample: {duration/len(df):.5f} seconds")

#     return df

# # Set a larger batch size for GPU processing
# BATCH_SIZE = 80  # You can experiment with this value

# # Assuming you have already created processed_df with 'cleaned_text' column
# processed_df = process_dataframe_with_embeddings(processed_df, model, BATCH_SIZE)

# print("Embeddings generation complete. 'embeddings' column added to processed_df.")

# # Optional: Free up GPU memory
# torch.cuda.empty_cache()

processed_df.head()

# Check for NaN or infinite values in the specified columns
columns_to_check = ['group_id', 'entity_name', 'entity_value_numeric', 'entity_unit_encoded', 'embeddings']

# Drop rows where 'entity_value_numeric' is NaN
processed_df.dropna(subset=['entity_value_numeric'], inplace=True)

# Check for NaN values in the DataFrame
nan_counts = processed_df.isna().sum()
print(nan_counts)

# Splitting the data into train and test sets
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(processed_df, test_size=0.2, random_state=42)
print("Train shape:", train_df.shape, "Test shape:", test_df.shape)

print(train_df.columns)
print(test_df.columns)

# import torch
# import torch.nn as nn
# from torch.cuda.amp import autocast
# from sklearn.metrics import f1_score, accuracy_score
# import numpy as np
# from tqdm import tqdm

# def evaluate_model(model, test_loader, device, print_examples_count=10):
#     model.eval()
#     all_combined_preds = []
#     all_combined_targets = []

#     # Separate storage for losses
#     all_value_preds = []
#     all_value_targets = []
#     all_unit_preds = []
#     all_unit_targets = []

#     value_criterion = nn.L1Loss()  # L1 loss (MAE)
#     unit_criterion = nn.CrossEntropyLoss()

#     test_value_loss = 0
#     test_unit_loss = 0

#     examples_printed = 0

#     with torch.no_grad():
#         for batch in tqdm(test_loader, desc="Evaluating on Test Data"):
#             text_embedding = batch['text_embedding'].to(device)
#             entity = batch['entity']
#             group = batch['group']
#             value = batch['value'].to(device)
#             unit = batch['unit'].to(device)

#             with autocast():
#                 value_pred, unit_pred = model(text_embedding, entity, group)

#             print(f"Value Predictions: {value_pred.cpu().numpy()}")
#             print(f"Unit Predictions: {torch.argmax(unit_pred, dim=1).cpu().numpy()}")
#             print(f"True Values: {value.cpu().numpy()}")
#             print(f"True Units: {unit.cpu().numpy()}")
#             break

#             # Calculate test losses
#             v_loss = value_criterion(value_pred, value)
#             u_loss = unit_criterion(unit_pred, unit)

#             test_value_loss += v_loss.item()
#             test_unit_loss += u_loss.item()

#             # Predictions
#             predicted_value = value_pred.cpu().numpy()
#             predicted_unit = torch.argmax(unit_pred, dim=1).cpu().numpy()

#             # Actual values
#             true_value = value.cpu().numpy()
#             true_unit = unit.cpu().numpy()

#             # Combine predictions and targets as tuples (for comparison)
#             combined_preds = [(v, u) for v, u in zip(predicted_value, predicted_unit)]
#             combined_targets = [(v, u) for v, u in zip(true_value, true_unit)]

#             all_combined_preds.extend(combined_preds)
#             all_combined_targets.extend(combined_targets)

#             # For separate metrics if needed
#             all_value_preds.extend(predicted_value)
#             all_value_targets.extend(true_value)
#             all_unit_preds.extend(predicted_unit)
#             all_unit_targets.extend(true_unit)

#             # Print predicted and true tuples for a limited number of examples
#             if examples_printed < print_examples_count:
#                 for pred, true in zip(combined_preds, combined_targets):
#                     print(f"Predicted: {pred} | True: {true}")
#                 examples_printed += len(combined_preds)
#                 if examples_printed >= print_examples_count:
#                     break

#     # Compute combined accuracy
#     correct_preds = sum([1 if pred == target else 0 for pred, target in zip(all_combined_preds, all_combined_targets)])
#     combined_accuracy = correct_preds / len(all_combined_preds)

#     # Compute F1 score
#     f1 = f1_score(
#         [1 if pred == target else 0 for pred, target in zip(all_combined_preds, all_combined_targets)],
#         [1] * len(all_combined_preds),
#         average='weighted'
#     )

#     # Print results
#     print("\nTest Set Evaluation:")
#     print(f"Test Value Loss (L1/MAE): {test_value_loss / len(test_loader):.4f}")
#     print(f"Test Unit Loss (CrossEntropy): {test_unit_loss / len(test_loader):.4f}")
#     print(f"Combined Accuracy (value + unit): {combined_accuracy:.4f}")
#     print(f"F1 Score (combined value + unit): {f1:.4f}")

#     return {
#         "test_value_loss": test_value_loss / len(test_loader),
#         "test_unit_loss": test_unit_loss / len(test_loader),
#         "combined_accuracy": combined_accuracy,
#         "f1_score": f1
#     }

# import os
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# import pandas as pd
# import numpy as np
# from tqdm import tqdm
# from sklearn.model_selection import train_test_split
# from torch.cuda.amp import autocast, GradScaler

# # Ensure CUDA is available
# assert torch.cuda.is_available(), "CUDA is not available. Please check your GPU setup."

# # Set environment variable for CUDA debugging
# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# class MultiEmbeddingDataset(Dataset):
#     def __init__(self, df):
#         self.text_embeddings = torch.tensor(df['embeddings'].tolist(), dtype=torch.float32)
#         self.entity_names = df['entity_name'].tolist()
#         self.group_ids = df['group_id'].tolist()
#         self.values = torch.tensor(df['entity_value_numeric'].values, dtype=torch.float32)
#         self.units = torch.tensor(df['entity_unit_encoded'].values, dtype=torch.long)

#     def __len__(self):
#         return len(self.text_embeddings)

#     def __getitem__(self, idx):
#         return {
#             'text_embedding': self.text_embeddings[idx],
#             'entity': self.entity_names[idx],
#             'group': self.group_ids[idx],
#             'value': self.values[idx],
#             'unit': self.units[idx]
#         }

# class MultiEmbeddingModel(nn.Module):
#     def __init__(self, entity_vocab, group_vocab, entity_embed_dim, group_embed_dim, text_embed_dim, mlp_hidden_dim, num_units):
#         super(MultiEmbeddingModel, self).__init__()
#         self.entity_embedding = nn.Embedding(len(entity_vocab), entity_embed_dim)
#         self.group_embedding = nn.Embedding(len(group_vocab), group_embed_dim)
#         self.entity_vocab = entity_vocab
#         self.group_vocab = group_vocab

#         total_embed_dim = entity_embed_dim + group_embed_dim + text_embed_dim

#         self.mlp = nn.Sequential(
#             nn.Linear(total_embed_dim, mlp_hidden_dim),
#             nn.BatchNorm1d(mlp_hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(0.3),

#             nn.Linear(mlp_hidden_dim, mlp_hidden_dim),
#             nn.BatchNorm1d(mlp_hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(0.3),

#             nn.Linear(mlp_hidden_dim, mlp_hidden_dim),
#             nn.ReLU()
#         )

#         self.value_head = nn.Linear(mlp_hidden_dim, 1)
#         self.unit_head = nn.Linear(mlp_hidden_dim, num_units)

#     def forward(self, text_embedding, entity, group):
#         entity_indices = torch.tensor([self.entity_vocab.get(e, 0) for e in entity], device=text_embedding.device)
#         group_indices = torch.tensor([self.group_vocab.get(g, 0) for g in group], device=text_embedding.device)

#         entity_embed = self.entity_embedding(entity_indices)
#         group_embed = self.group_embedding(group_indices)

#         combined = torch.cat([text_embedding, entity_embed, group_embed], dim=1)
#         hidden = self.mlp(combined)

#         value_pred = self.value_head(hidden).squeeze(-1)
#         unit_pred = self.unit_head(hidden)

#         return value_pred, unit_pred

# def train_model(model, train_loader, val_loader, num_epochs, device, patience=5, accumulation_steps=4, value_tolerance=0.1):
#     model.to(device)
#     optimizer = optim.Adam(model.parameters())
#     value_criterion = nn.L1Loss(reduction='none')  # L1Loss for MAE
#     unit_criterion = nn.CrossEntropyLoss(reduction='none')
#     scaler = GradScaler()
#     best_val_accuracy = -float('inf')
#     epochs_without_improvement = 0

#     def combined_accuracy_loss(value_pred, unit_logits, true_value, true_unit):
#         value_loss = value_criterion(value_pred, true_value)
#         unit_loss = unit_criterion(unit_logits, true_unit)

#         value_correct = (value_pred - true_value).abs() <= (true_value * value_tolerance)
#         unit_pred = torch.argmax(unit_logits, dim=1)
#         unit_correct = (unit_pred == true_unit)

#         # Both value and unit must be correct for a prediction to be considered correct
#         combined_correct = value_correct & unit_correct

#         # Calculate total loss
#         total_loss = 0.7*value_loss + 0.3*unit_loss

#         # Accuracy is now based on both value and unit being correct simultaneously
#         accuracy = combined_correct.float().mean()

#         return total_loss.mean(), accuracy

#     for epoch in range(num_epochs):
#         model.train()
#         train_loss = 0
#         train_accuracy = 0
#         optimizer.zero_grad()

#         for i, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")):
#             text_embedding = batch['text_embedding'].to(device)
#             entity = batch['entity']
#             group = batch['group']
#             true_value = batch['value'].to(device)
#             true_unit = batch['unit'].to(device)

#             try:
#                 with autocast():
#                     value_pred, unit_logits = model(text_embedding, entity, group)
#                     loss, accuracy = combined_accuracy_loss(value_pred, unit_logits, true_value, true_unit)

#                 scaler.scale(loss / accumulation_steps).backward()

#                 if (i + 1) % accumulation_steps == 0:
#                     scaler.step(optimizer)
#                     scaler.update()
#                     optimizer.zero_grad()

#                 train_loss += loss.item()
#                 train_accuracy += accuracy.item()

#             except Exception as e:
#                 print(f"Error in training step {i}: {e}")
#                 continue

#         train_loss /= len(train_loader)
#         train_accuracy /= len(train_loader)
#         print(f"Epoch {epoch+1}/{num_epochs}")
#         print(f"Train Loss: {train_loss:.4f}")
#         print(f"Train Accuracy: {train_accuracy:.4f}")

#         # Validation
#         model.eval()
#         val_loss = 0
#         val_accuracy = 0

#         with torch.no_grad():
#             for batch in val_loader:
#                 text_embedding = batch['text_embedding'].to(device)
#                 entity = batch['entity']
#                 group = batch['group']
#                 true_value = batch['value'].to(device)
#                 true_unit = batch['unit'].to(device)

#                 try:
#                     with autocast():
#                         value_pred, unit_logits = model(text_embedding, entity, group)
#                         loss, accuracy = combined_accuracy_loss(value_pred, unit_logits, true_value, true_unit)

#                     val_loss += loss.item()
#                     val_accuracy += accuracy.item()

#                 except Exception as e:
#                     print(f"Error during validation: {e}")
#                     continue

#         val_loss /= len(val_loader)
#         val_accuracy /= len(val_loader)
#         print(f"Val Loss: {val_loss:.4f}")
#         print(f"Val Accuracy: {val_accuracy:.4f}")

#         if val_accuracy > best_val_accuracy:
#             best_val_accuracy = val_accuracy
#             epochs_without_improvement = 0
#             torch.save(model.state_dict(), 'best_model.pt')
#         else:
#             epochs_without_improvement += 1
#             if epochs_without_improvement == patience:
#                 print(f"Early stopping triggered after {epoch+1} epochs")
#                 break

#         torch.cuda.empty_cache()

#     return model

# def main():
#     try:
#         print("PyTorch version:", torch.__version__)
#         print("CUDA available:", torch.cuda.is_available())
#         print("CUDA version:", torch.version.cuda)

#         # Ensure dataset is available
#         assert 'processed_df' in globals(), "Dataset 'processed_df' not found."

#         print("Splitting data...")
#         train_df, test_df = train_test_split(processed_df, test_size=0.2, random_state=42)
#         print("Train shape:", train_df.shape, "Test shape:", test_df.shape)

#         device = torch.device('cuda')
#         print("Using device:", device)

#         print("Creating datasets...")
#         train_dataset = MultiEmbeddingDataset(train_df)
#         test_dataset = MultiEmbeddingDataset(test_df)
#         print("Datasets created successfully")

#         print("Creating dataloaders...")
#         train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)
#         test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)
#         print("Dataloaders created successfully")

#         print("Initializing model...")
#         entity_vocab = {entity: idx for idx, entity in enumerate(processed_df['entity_name'].unique())}
#         group_vocab = {group: idx for idx, group in enumerate(processed_df['group_id'].unique())}
#         entity_embed_dim = 32
#         group_embed_dim = 16
#         text_embed_dim = 384  # As per your embedding size
#         mlp_hidden_dim = 128
#         num_units = len(processed_df['entity_unit_encoded'].unique())

#         model = MultiEmbeddingModel(entity_vocab, group_vocab, entity_embed_dim, group_embed_dim, text_embed_dim, mlp_hidden_dim, num_units)
#         print("Model initialized successfully")

#         # Print model summary
#         print(model)

#         print("Starting model training...")
#         train_model(model, train_loader, test_loader, num_epochs=10, device=device, patience=5)
#         print("Model training completed")

#         # Load the best model checkpoint for evaluation
#         model.load_state_dict(torch.load('best_model.pt'))
#         model.to(device)

#         print("Starting model evaluation...")
#         metrics = evaluate_model(model, test_loader, device)
#         print("Evaluation completed successfully")

#         # Print the evaluation metrics
#         print(metrics)

#         # Final memory cleanup
#         torch.cuda.empty_cache()

#     except Exception as e:
#         print(f"An error occurred: {str(e)}")
#         import traceback
#         print(traceback.format_exc())

# if __name__ == '__main__':
#     main()

import re
from collections import defaultdict
import pandas as pd
import dask.dataframe as dd
from dask.distributed import Client
from tqdm import tqdm

class TrieNode:
    def __init__(self):
        self.children = defaultdict(TrieNode)
        self.is_end = False
        self.standard_unit = None

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word, standard_unit):
        node = self.root
        for char in word:
            node = node.children[char]
        node.is_end = True
        node.standard_unit = standard_unit

    def find_longest_prefix(self, word):
        node = self.root
        last_match = None
        last_match_length = 0
        for i, char in enumerate(word):
            if char not in node.children:
                break
            node = node.children[char]
            if node.is_end:
                last_match = node.standard_unit
                last_match_length = i + 1
        return last_match, last_match_length

# Define unit mappings
unit_mappings = {
    'item_weight': {
        'gram': {'gram', 'g', 'gm', 'grams', 'gr', 'grm', 'grms'},
        'kilogram': {'kilogram', 'kg', 'kgs', 'kilograms', 'kilo', 'kilos'},
        'microgram': {'microgram', 'µg', 'mcg', 'micrograms'},
        'milligram': {'milligram', 'mg', 'mgs', 'milligrams', 'milli-gram'},
        'ounce': {'ounce', 'oz', 'ounces'},
        'pound': {'pound', 'lb', 'lbs', 'pounds'},
        'ton': {'ton', 't', 'tons', 'tonne', 'tonnes'}
    },
    'width': {
        'centimetre': {'centimetre', 'cm', 'centimeters', 'centimeter', 'cms'},
        'foot': {'foot', 'ft', 'feet', "'"},
        'inch': {'inch', 'in', '"', 'inches'},
        'metre': {'metre', 'm', 'meter', 'meters'},
        'millimetre': {'millimetre', 'mm', 'millimeter', 'millimeters', 'mms'},
        'yard': {'yard', 'yd', 'yards'}
    },
    'depth': {
        'centimetre': {'centimetre', 'cm', 'centimeters', 'centimeter', 'cms'},
        'foot': {'foot', 'ft', 'feet', "'"},
        'inch': {'inch', 'in', '"', 'inches'},
        'metre': {'metre', 'm', 'meter', 'meters'},
        'millimetre': {'millimetre', 'mm', 'millimeter', 'millimeters', 'mms'},
        'yard': {'yard', 'yd', 'yards'}
    },
    'height': {
        'centimetre': {'centimetre', 'cm', 'centimeters', 'centimeter', 'cms'},
        'foot': {'foot', 'ft', 'feet', "'"},
        'inch': {'inch', 'in', '"', 'inches'},
        'metre': {'metre', 'm', 'meter', 'meters'},
        'millimetre': {'millimetre', 'mm', 'millimeter', 'millimeters', 'mms'},
        'yard': {'yard', 'yd', 'yards'}
    },
    'maximum_weight_recommendation': {
        'gram': {'gram', 'g', 'gm', 'grams', 'gr', 'grm', 'grms'},
        'kilogram': {'kilogram', 'kg', 'kgs', 'kilograms', 'kilo', 'kilos'},
        'microgram': {'microgram', 'µg', 'mcg', 'micrograms'},
        'milligram': {'milligram', 'mg', 'mgs', 'milligrams', 'milli-gram'},
        'ounce': {'ounce', 'oz', 'ounces'},
        'pound': {'pound', 'lb', 'lbs', 'pounds'},
        'ton': {'ton', 't', 'tons', 'tonne', 'tonnes'}
    },
    'voltage': {
        'kilovolt': {'kilovolt', 'kv', 'kilovolts', 'kilo-volt'},
        'millivolt': {'millivolt', 'mv', 'millivolts'},
        'volt': {'volt', 'v', 'volts'}
    },
    'wattage': {
        'kilowatt': {'kilowatt', 'kw', 'kilowatts', 'kilo-watt'},
        'watt': {'watt', 'w', 'watts'}
    },
    'item_volume': {
        'centilitre': {'centilitre', 'cl', 'centilitres', 'centiliter', 'centiliters'},
        'cubic foot': {'cubic foot', 'ft³', 'cu ft', 'ft^3', 'cubic feet'},
        'cubic inch': {'cubic inch', 'in³', 'cu in', 'in^3', 'cubic inches'},
        'cup': {'cup', 'cups'},
        'decilitre': {'decilitre', 'dl', 'deciliter', 'decilitres', 'deciliters'},
        'fluid ounce': {'fluid ounce', 'fl oz', 'fl. oz.', 'fluid ounces', 'fluid-oz'},
        'gallon': {'gallon', 'gal', 'gallons'},
        'imperial gallon': {'imperial gallon', 'imp gal', 'imp gallons', 'imperial gallons'},
        'litre': {'litre', 'l', 'litres', 'liter', 'liters'},
        'microlitre': {'microlitre', 'µl', 'microliters', 'microliter', 'microlitres'},
        'millilitre': {'millilitre', 'ml', 'milliliters', 'millilitres', 'milli-litre', 'milli-liter'},
        'pint': {'pint', 'pt', 'pints'},
        'quart': {'quart', 'qt', 'quarts'}
    }
}


def build_trie(unit_mappings):
    trie = Trie()
    for entity, units in unit_mappings.items():
        for standard_unit, variations in units.items():
            for variation in variations:
                trie.insert(variation.lower(), standard_unit)
    return trie

unit_trie = build_trie(unit_mappings)

def convert_units(text, entity_name):
    words = text.split()
    result = []
    i = 0
    while i < len(words):
        match, length = unit_trie.find_longest_prefix(words[i].lower())
        if match:
            result.append(match)
            i += 1
        else:
            result.append(words[i])
            i += 1
    return ' '.join(result)

def process_row(row):
    text = row['cleaned_text']
    entity_name = row['entity_name']
    processed_text = convert_units(text, entity_name)
    return processed_text

def process_dataframe(df, num_partitions=100):
    ddf = dd.from_pandas(df, npartitions=num_partitions)
    processed_texts = ddf.apply(process_row, axis=1, meta=('processed_text', 'object'))
    return processed_texts.compute()

# Usage
if __name__ == '__main__':
    # Set up Dask client
    client = Client()  # This will use all available cores by default


    # Process the DataFrame
    processed_df['processed_text'] = process_dataframe(processed_df)

    # Save the result
    # df.to_csv('processed_data.csv', index=False)

    # Close the Dask client
    client.close()

processed_df['processed_text']

processed_df['cleaned_text']

